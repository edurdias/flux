{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Flux Documentation","text":"<p>Flux documentation is AI generated. If you find any issues, please let us know.</p>"},{"location":"#introduction","title":"Introduction","text":""},{"location":"#overview-of-flux","title":"Overview of Flux","text":""},{"location":"#key-features","title":"Key Features","text":"<ul> <li>High-Performance Task Execution</li> <li>Fault-Tolerance</li> <li>Durable Execution</li> <li>Workflow Controls</li> <li>API Integration</li> <li>Security</li> <li>Development Features</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<ul> <li>Requirements</li> <li>Installation Guide</li> <li>Quick Setup</li> </ul>"},{"location":"#basic-concepts","title":"Basic Concepts","text":"<ul> <li>Workflows</li> <li>Tasks</li> <li>Execution Context</li> <li>Events</li> </ul>"},{"location":"#quick-start-guide","title":"Quick Start Guide","text":"<ul> <li>First Workflow</li> <li>Running Workflows</li> </ul>"},{"location":"#core-concepts","title":"Core Concepts","text":""},{"location":"#workflow-management","title":"Workflow Management","text":"<ul> <li>Creating Workflows</li> <li>Workflow Lifecycle</li> <li>Workflow States</li> </ul>"},{"location":"#task-system","title":"Task System","text":"<ul> <li>Task Creation</li> <li>Task Options</li> <li>Task Composition</li> <li>Error Handling</li> <li>Built-in Tasks</li> </ul>"},{"location":"#execution-model","title":"Execution Model","text":"<ul> <li>Local Execution</li> <li>API-based Execution</li> <li>Execution Context</li> <li>Paused Workflows</li> <li>State Management</li> <li>Event System</li> </ul>"},{"location":"#error-handling-recovery","title":"Error Handling &amp; Recovery","text":"<ul> <li>Task-Level Error Handling</li> <li>Retry Mechanisms</li> <li>Fallback Strategies</li> <li>Rollback Operations</li> <li>Timeout Handling</li> <li>Task Caching</li> </ul>"},{"location":"#advanced-features","title":"Advanced Features","text":""},{"location":"#task-patterns","title":"Task Patterns","text":"<ul> <li>Parallel Execution</li> <li>Pipeline Processing</li> <li>Task Mapping</li> <li>Graph</li> <li>Performance Considerations</li> </ul>"},{"location":"#workflow-controls","title":"Workflow Controls","text":"<ul> <li>Workflow Pause Points</li> <li>Workflow Replay</li> <li>Subworkflow Support</li> </ul>"},{"location":"#appendix","title":"Appendix","text":"<ul> <li>Examples in the Repository</li> <li>API Reference</li> <li>Version History</li> <li>Contributing Guidelines</li> </ul>"},{"location":"advanced-features/task-patterns/","title":"Task Patterns","text":""},{"location":"advanced-features/task-patterns/#parallel-execution","title":"Parallel Execution","text":"<p>Parallel execution allows multiple tasks to run concurrently, improving performance for independent operations.</p>"},{"location":"advanced-features/task-patterns/#using-the-parallel-task","title":"Using the Parallel Task","text":"<pre><code>from flux import task, workflow, ExecutionContext\nfrom flux.tasks import parallel\n\n@task\nasync def say_hi(name: str):\n    return f\"Hi, {name}\"\n\n@task\nasync def say_hello(name: str):\n    return f\"Hello, {name}\"\n\n@task\nasync def say_hola(name: str):\n    return f\"Hola, {name}\"\n\n@workflow\nasync def parallel_workflow(ctx: ExecutionContext[str]):\n    results = await parallel(\n        say_hi(ctx.input),\n        say_hello(ctx.input),\n        say_hola(ctx.input)\n    )\n    return results\n</code></pre> <p>Key features: - Executes tasks concurrently using ThreadPoolExecutor - Automatically manages thread pool based on CPU cores - Returns results in order of task definition - Handles failures in individual tasks</p>"},{"location":"advanced-features/task-patterns/#pipeline-processing","title":"Pipeline Processing","text":"<p>Pipelines chain multiple tasks together, passing results from one task to the next.</p>"},{"location":"advanced-features/task-patterns/#using-the-pipeline-task","title":"Using the Pipeline Task","text":"<pre><code>from flux import task, workflow, ExecutionContext\nfrom flux.tasks import pipeline\n\n@task\nasync def multiply_by_two(x):\n    return x * 2\n\n@task\nasync def add_three(x):\n    return x + 3\n\n@task\nasync def square(x):\n    return x * x\n\n@workflow\nasync def pipeline_workflow(ctx: ExecutionContext[int]):\n    result = await pipeline(\n        multiply_by_two,\n        add_three,\n        square,\n        input=ctx.input\n    )\n    return result\n</code></pre> <p>Key features: - Sequential task execution - Automatic result passing between tasks - Clear data transformation flow - Error propagation through the pipeline</p>"},{"location":"advanced-features/task-patterns/#task-mapping","title":"Task Mapping","text":"<p>Task mapping applies a single task to multiple inputs in parallel.</p>"},{"location":"advanced-features/task-patterns/#basic-task-mapping","title":"Basic Task Mapping","text":"<pre><code>from flux import task, workflow, ExecutionContext\n\n@task\nasync def process_item(item: str):\n    return item.upper()\n\n@workflow\nasync def mapping_workflow(ctx: ExecutionContext[list[str]]):\n    # Process multiple items in parallel\n    results = await process_item.map(ctx.input)\n    return results\n</code></pre>"},{"location":"advanced-features/task-patterns/#complex-mapping","title":"Complex Mapping","text":"<pre><code>@task\nasync def count(to: int):\n    return [i for i in range(0, to + 1)]\n\n@workflow\nasync def task_map_workflow(ctx: ExecutionContext[int]):\n    # Generate sequences in parallel\n    results = await count.map(list(range(0, ctx.input)))\n    return len(results)\n</code></pre> <p>Key features: - Parallel processing of multiple inputs - Automatic thread pool management - Result aggregation - Error handling for individual mappings</p>"},{"location":"advanced-features/task-patterns/#graph","title":"Graph","text":"<p>Graphs allow complex task dependencies and conditional execution paths.</p>"},{"location":"advanced-features/task-patterns/#basic-graph","title":"Basic Graph","text":"<pre><code>from flux import task, workflow, ExecutionContext\nfrom flux.tasks import Graph\n\n@task\nasync def get_name(input: str) -&gt; str:\n    return input\n\n@task\nasync def say_hello(name: str) -&gt; str:\n    return f\"Hello, {name}\"\n\n@workflow\nasync def graph_workflow(ctx: ExecutionContext[str]):\n    hello = (\n        Graph(\"hello_world\")\n        .add_node(\"get_name\", get_name)\n        .add_node(\"say_hello\", say_hello)\n        .add_edge(\"get_name\", \"say_hello\")\n        .start_with(\"get_name\")\n        .end_with(\"say_hello\")\n    )\n    return await hello(ctx.input)\n</code></pre>"},{"location":"advanced-features/task-patterns/#conditional-graph-execution","title":"Conditional Graph Execution","text":"<pre><code>@workflow\nasync def conditional_graph_workflow(ctx: ExecutionContext):\n    workflow = (\n        Graph(\"conditional_flow\")\n        .add_node(\"validate\", validate_data)\n        .add_node(\"process\", process_data)\n        .add_node(\"error\", handle_error)\n        .add_edge(\"validate\", \"process\",\n                 condition=lambda result: result.get(\"valid\"))\n        .add_edge(\"validate\", \"error\",\n                 condition=lambda result: not result.get(\"valid\"))\n        .start_with(\"validate\")\n        .end_with(\"process\")\n        .end_with(\"error\")\n    )\n    return await workflow(ctx.input)\n</code></pre> <p>Key features: - Define complex task dependencies - Conditional execution paths - Automatic validation of graph structure - Clear visualization of workflow logic - Flexible error handling paths</p>"},{"location":"advanced-features/task-patterns/#pattern-selection-guidelines","title":"Pattern Selection Guidelines","text":"<p>Choose the appropriate pattern based on your needs:</p> <ol> <li>Parallel Execution when:</li> <li>Tasks are independent</li> <li>You want to improve performance</li> <li> <p>Order of execution doesn't matter</p> </li> <li> <p>Pipeline when:</p> </li> <li>Tasks form a sequential chain</li> <li>Each task depends on previous results</li> <li> <p>You need clear data transformation steps</p> </li> <li> <p>Task Mapping when:</p> </li> <li>Same operation applies to multiple items</li> <li>Items can be processed independently</li> <li> <p>You want to parallelize processing</p> </li> <li> <p>Graph when:</p> </li> <li>You have complex task dependencies</li> <li>You need conditional execution paths</li> <li>Workflow has multiple possible paths</li> </ol>"},{"location":"advanced-features/task-patterns/#performance-considerations","title":"Performance Considerations","text":""},{"location":"advanced-features/task-patterns/#parallel-execution-performance","title":"Parallel Execution Performance","text":""},{"location":"advanced-features/task-patterns/#thread-pool-management","title":"Thread Pool Management","text":"<pre><code>from flux.tasks import parallel\n\n@workflow\nasync def parallel_workflow(ctx: ExecutionContext):\n    # Tasks are executed using ThreadPoolExecutor\n    # Number of workers = CPU cores available\n    results = await parallel(\n        task1(),\n        task2(),\n        task3()\n    )\n</code></pre> <p>Key considerations: - Uses Python's asyncio for concurrent task execution - Best for I/O-bound tasks (network requests, file operations) - All tasks start simultaneously, consuming resources immediately</p> <p>Optimization tips: 1. Group tasks appropriately: <pre><code># Less efficient (too granular)\nresults = await parallel(\n    task1(item1),\n    task1(item2),\n    task1(item3),\n    task2(item1),\n    task2(item2),\n    task2(item3)\n)\n\n# More efficient (better grouping)\ngroup1 = await parallel(\n    task1(item1),\n    task1(item2),\n    task1(item3)\n)\ngroup2 = await parallel(\n    task2(item1),\n    task2(item2),\n    task2(item3)\n)\n</code></pre></p> <ol> <li>Consider resource constraints: <pre><code># Resource-intensive tasks should be grouped appropriately\nresults = await parallel(\n    lambda: heavy_task1(),  # Uses significant memory\n    lambda: light_task(),   # Minimal resource usage\n    lambda: heavy_task2()   # Uses significant memory\n)\n</code></pre></li> </ol>"},{"location":"advanced-features/task-patterns/#pipeline-performance","title":"Pipeline Performance","text":"<p>Pipeline execution is sequential, making performance dependent on the slowest task.</p> <pre><code>@workflow\nasync def pipeline_workflow(ctx: ExecutionContext):\n    result = await pipeline(\n        fast_task,      # 0.1s\n        slow_task,      # 2.0s\n        medium_task,    # 0.5s\n        input=ctx.input\n    )\n    # Total time \u2248 2.6s\n</code></pre> <p>Optimization tips: 1. Order tasks efficiently:    - Put quick validation tasks first    - Group data transformation tasks    - Place heavy processing tasks last</p> <ol> <li>Balance task granularity: <pre><code># Less efficient (too granular)\nresult = await pipeline(\n    validate_input,\n    transform_data,\n    process_part1,\n    process_part2,\n    process_part3,\n    save_result,\n    input=ctx.input\n)\n\n# More efficient (better grouping)\nresult = await pipeline(\n    validate_and_transform,  # Combined validation and transformation\n    process_all_parts,      # Combined processing\n    save_result,\n    input=ctx.input\n)\n</code></pre></li> </ol>"},{"location":"advanced-features/task-patterns/#task-mapping-performance","title":"Task Mapping Performance","text":"<p>Task mapping parallelizes the same operation across multiple inputs.</p> <pre><code>@task\nasync def process_item(item: str):\n    return item.upper()\n\n@workflow\nasync def mapping_workflow(ctx: ExecutionContext):\n    # Be mindful of the input size\n    results = await process_item.map(large_input_list)\n</code></pre> <p>Key considerations: - Built on top of asyncio.gather for concurrent execution - Memory usage scales with input size - All results are collected in memory</p> <p>Optimization tips: 1. Batch processing for large datasets: <pre><code>@workflow\nasync def optimized_mapping(ctx: ExecutionContext):\n    # Process in smaller batches\n    batch_size = 1000\n    results = []\n    for i in range(0, len(ctx.input), batch_size):\n        batch = ctx.input[i:i + batch_size]\n        batch_results = await process_item.map(batch)\n        results.extend(batch_results)\n</code></pre></p> <ol> <li>Memory-efficient processing: <pre><code>@workflow\nasync def memory_efficient_mapping(ctx: ExecutionContext):\n    # Process and store results incrementally\n    results = []\n    for batch in chunk_generator(ctx.input, size=1000):\n        batch_results = await process_item.map(batch)\n        # Process or store results before next batch\n        await store_results(batch_results)\n</code></pre></li> </ol>"},{"location":"advanced-features/task-patterns/#graph-performance","title":"Graph Performance","text":"<p>Graph execution performance depends on task dependencies and conditions.</p> <pre><code>@workflow\nasync def graph_workflow(ctx: ExecutionContext):\n    workflow = (\n        Graph(\"optimized_flow\")\n        .add_node(\"validate\", quick_validation)\n        .add_node(\"process\", heavy_processing)\n        .add_node(\"error\", handle_error)\n        .add_edge(\"validate\", \"process\",\n                 condition=lambda r: r.get(\"valid\"))\n        .add_edge(\"validate\", \"error\",\n                 condition=lambda r: not r.get(\"valid\"))\n        .start_with(\"validate\")\n        .end_with(\"process\")\n        .end_with(\"error\")\n    )\n</code></pre> <p>Optimization tips: 1. Optimize graph structure:    - Place validation and lightweight tasks early    - Group related tasks to minimize edge complexity    - Use conditions to skip unnecessary tasks</p> <ol> <li>Balance between complexity and performance: <pre><code># Less efficient (too many edges)\ngraph = (\n    Graph(\"complex\")\n    .add_node(\"A\", task_a)\n    .add_node(\"B\", task_b)\n    .add_node(\"C\", task_c)\n    .add_edge(\"A\", \"B\")\n    .add_edge(\"A\", \"C\")\n    .add_edge(\"B\", \"C\")\n)\n\n# More efficient (simplified structure)\ngraph = (\n    Graph(\"optimized\")\n    .add_node(\"A\", task_a)\n    .add_node(\"BC\", combined_task_bc)\n    .add_edge(\"A\", \"BC\")\n)\n</code></pre></li> </ol>"},{"location":"advanced-features/task-patterns/#general-performance-tips","title":"General Performance Tips","text":"<ol> <li>Resource Management</li> <li>Monitor memory usage in parallel operations</li> <li>Use appropriate batch sizes for large datasets</li> <li> <p>Consider I/O vs CPU-bound task characteristics</p> </li> <li> <p>Task Granularity</p> </li> <li>Balance between too fine and too coarse</li> <li>Group related operations when possible</li> <li> <p>Split very large tasks into manageable pieces</p> </li> <li> <p>Error Handling</p> </li> <li>Implement early validation to fail fast</li> <li>Use appropriate timeouts</li> <li> <p>Consider the cost of retries and fallbacks</p> </li> <li> <p>State Management</p> </li> <li>Be mindful of data size in context</li> <li>Implement cleanup for temporary data</li> <li>Use appropriate storage strategies for large results</li> </ol>"},{"location":"advanced-features/workflow-controls/","title":"Workflow Controls","text":""},{"location":"advanced-features/workflow-controls/#workflow-pause-points","title":"Workflow Pause Points","text":"<p>Flux allows you to introduce pause points in your workflows, enabling manual verification, approval steps, or integration with external systems before continuation.</p>"},{"location":"advanced-features/workflow-controls/#basic-pause-functionality","title":"Basic Pause Functionality","text":"<pre><code>from flux import workflow, ExecutionContext\nfrom flux.decorators import task\nfrom flux.tasks import pause\n\n@task\nasync def process_data():\n    # Process data\n    return \"Data processed\"\n\n@workflow\nasync def pause_workflow(ctx: WorkflowExecutionContext):\n    result = await process_data()\n\n    # Pause workflow execution until resumed\n    await pause(\"wait_for_approval\")\n\n    return result + \" and approved\"\n\n# First execution - runs until pause point\nctx = pause_workflow.run()\n\n# Resume execution from pause point\nctx = pause_workflow.run(execution_id=ctx.execution_id)\n</code></pre>"},{"location":"advanced-features/workflow-controls/#multiple-pause-points","title":"Multiple Pause Points","text":"<p>Workflows can contain multiple pause points, giving you fine-grained control over execution flow:</p> <pre><code>@workflow\nasync def multi_stage_workflow(ctx: WorkflowExecutionContext):\n    # Stage 1\n    data = await prepare_data()\n    await pause(\"verify_data\")\n\n    # Stage 2\n    processed = await process_data(data)\n    await pause(\"review_processing\")\n\n    # Stage 3\n    result = await finalize_data(processed)\n    return result\n</code></pre>"},{"location":"advanced-features/workflow-controls/#checking-pause-state","title":"Checking Pause State","text":"<p>You can check if a workflow is paused through the context object:</p> <pre><code>ctx = workflow.run()\nif ctx.paused:\n    print(f\"Workflow is paused.\")\n</code></pre>"},{"location":"advanced-features/workflow-controls/#workflow-replay","title":"Workflow Replay","text":"<p>Flux automatically handles workflow replay, maintaining consistency and idempotency.</p>"},{"location":"advanced-features/workflow-controls/#deterministic-execution","title":"Deterministic Execution","text":"<pre><code>@workflow\nasync def deterministic_workflow():\n    # These tasks will produce the same results\n    # when the workflow is replayed\n    start = await now()\n    await uuid4()\n    await randint(1, 5)\n    await randrange(1, 10)\n    end = await now()\n    return end - start\n\n# First execution\nctx1 = deterministic_workflow.run()\n\n# Replay execution\nctx2 = deterministic_workflow.run(execution_id=ctx1.execution_id)\n# ctx1.output == ctx2.output\n</code></pre>"},{"location":"advanced-features/workflow-controls/#subworkflows","title":"Subworkflows","text":"<p>Break down complex workflows into manageable, reusable components using subworkflows.</p>"},{"location":"advanced-features/workflow-controls/#basic-subworkflow","title":"Basic Subworkflow","text":"<pre><code>from flux import call\n\n@workflow\nasync def sub_workflow(ctx: WorkflowExecutionContext[str]):\n    result = await some_task(ctx.input)\n    return result\n\n@workflow\nasync def main_workflow(ctx: WorkflowExecutionContext[str]):\n    # Call subworkflow\n    result = await call(sub_workflow, ctx.input)\n    return result\n</code></pre>"},{"location":"advanced-features/workflow-controls/#parallel-subworkflows","title":"Parallel Subworkflows","text":"<pre><code>@workflow\nasync def get_stars_workflow(ctx: WorkflowExecutionContext[str]):\n    repo_info = await get_repo_info(ctx.input)\n    return repo_info[\"stargazers_count\"]\n\n@workflow\nasync def parallel_subflows(ctx: WorkflowExecutionContext[list[str]]):\n    if not ctx.input:\n        raise TypeError(\"Repository list cannot be empty\")\n\n    repos = ctx.input\n    stars = {}\n\n    # Execute subworkflows in parallel\n    responses = await get_stars_workflow.map(repos)\n\n    # Collect results\n    return {\n        repo: response.output\n        for repo, response in zip(repos, responses)\n    }\n</code></pre>"},{"location":"advanced-features/workflow-controls/#subworkflow-composition","title":"Subworkflow Composition","text":"<pre><code>@workflow\nasync def process_workflow(ctx: WorkflowExecutionContext):\n    # Sequential subworkflow execution\n    data = await call(fetch_data_workflow)\n    processed = await call(transform_workflow, data)\n    result = await call(save_workflow, processed)\n    return result\n</code></pre>"},{"location":"core-concepts/error-handling/","title":"Error Handling &amp; Recovery","text":""},{"location":"core-concepts/error-handling/#error-types","title":"Error Types","text":"<p>Flux provides several specialized error types for different scenarios:</p> <pre><code>from flux.errors import (\n    ExecutionError,           # Base error for execution issues\n    RetryError,              # Error during retry operations\n    ExecutionTimeoutError,   # Timeout during execution\n    WorkflowNotFoundError    # Workflow lookup failures\n)\n</code></pre>"},{"location":"core-concepts/error-handling/#task-level-error-handling","title":"Task-Level Error Handling","text":""},{"location":"core-concepts/error-handling/#retry-mechanism","title":"Retry Mechanism","text":"<pre><code>@task.with_options(\n    retry_max_attempts=3,    # Maximum retry attempts\n    retry_delay=1,          # Initial delay in seconds\n    retry_backoff=2         # Multiply delay by this factor each retry\n)\nasync def task_with_retries():\n    # First attempt fails: wait 1 second\n    # Second attempt fails: wait 2 seconds\n    # Third attempt fails: task fails\n    if random.random() &lt; 0.5:\n        raise ValueError(\"Task failed\")\n    return \"success\"\n</code></pre>"},{"location":"core-concepts/error-handling/#fallback-strategy","title":"Fallback Strategy","text":"<pre><code>async def fallback_handler(input_data):\n    return \"fallback result\"\n\n@task.with_options(\n    fallback=fallback_handler,\n    retry_max_attempts=3\n)\nasync def task_with_fallback(input_data):\n    # If all retries fail, fallback_handler is called\n    if not validate(input_data):\n        raise ValueError(\"Invalid data\")\n    return process(input_data)\n</code></pre>"},{"location":"core-concepts/error-handling/#rollback-operations","title":"Rollback Operations","text":"<pre><code>async def rollback_handler(input_data):\n    # Clean up any partial changes\n    cleanup_resources()\n\n@task.with_options(rollback=rollback_handler)\nasync def task_with_rollback(input_data):\n    # If task fails, rollback_handler is called\n    # before propagating the error\n    result = complex_operation(input_data)\n    if not verify(result):\n        raise ValueError(\"Verification failed\")\n    return result\n</code></pre>"},{"location":"core-concepts/error-handling/#combining-error-handling-strategies","title":"Combining Error Handling Strategies","text":"<pre><code>@task.with_options(\n    retry_max_attempts=3,\n    retry_delay=1,\n    retry_backoff=2,\n    fallback=fallback_handler,\n    rollback=rollback_handler,\n    timeout=30\n)\nasync def task_with_full_error_handling():\n    # 1. Task executes with timeout of 30 seconds\n    # 2. On failure, retries up to 3 times\n    # 3. If retries fail, rollback is executed\n    # 4. Finally, fallback is called\n    pass\n</code></pre>"},{"location":"core-concepts/error-handling/#workflow-level-error-handling","title":"Workflow-Level Error Handling","text":""},{"location":"core-concepts/error-handling/#try-except-pattern","title":"Try-Except Pattern","text":"<pre><code>@workflow\nasync def error_handling_workflow(ctx: WorkflowExecutionContext):\n    try:\n        result = await risky_task()\n        return result\n    except ExecutionError as e:\n        # Handle execution-specific errors\n        print(f\"Execution failed: {e.inner_exception}\")\n        raise\n    except Exception as e:\n        # Handle unexpected errors\n        print(f\"Unexpected error: {e}\")\n        raise\n</code></pre>"},{"location":"core-concepts/error-handling/#timeout-handling","title":"Timeout Handling","text":"<pre><code>@task.with_options(timeout=30)\nasync def long_running_task():\n    # Task will raise ExecutionTimeoutError after 30 seconds\n    await asyncio.sleep(35)\n\n@workflow\nasync def timeout_workflow(ctx: WorkflowExecutionContext):\n    try:\n        result = await long_running_task()\n        return result\n    except ExecutionTimeoutError as e:\n        # Handle timeout specifically\n        print(f\"Task timed out: {e.timeout}s\")\n        return fallback_value\n</code></pre>"},{"location":"core-concepts/error-handling/#error-recovery","title":"Error Recovery","text":""},{"location":"core-concepts/error-handling/#automatic-state-recovery","title":"Automatic State Recovery","text":"<p>Flux automatically maintains execution state, allowing workflows to resume from their last successful state:</p> <pre><code># Initial execution that might fail\nctx = await my_workflow.run(\"input_data\")\n\n# Resume from last successful state\n# No need to track state manually\nctx = await my_workflow.run(execution_id=ctx.execution_id)\n</code></pre>"},{"location":"core-concepts/error-handling/#error-events","title":"Error Events","text":"<p>Monitor error-related events to track failure patterns:</p> <pre><code>async def analyze_errors(ctx: WorkflowExecutionContext):\n    error_events = [\n        event for event in ctx.events\n        if event.type in [\n            ExecutionEventType.TASK_FAILED,\n            ExecutionEventType.TASK_RETRY_FAILED,\n            ExecutionEventType.TASK_FALLBACK_FAILED,\n            ExecutionEventType.WORKFLOW_FAILED\n        ]\n    ]\n    return error_events\n</code></pre>"},{"location":"core-concepts/error-handling/#best-practices","title":"Best Practices","text":""},{"location":"core-concepts/error-handling/#1-layer-your-error-handling","title":"1. Layer Your Error Handling","text":"<pre><code>@task.with_options(\n    retry_max_attempts=3,\n    fallback=fallback_handler\n)\nasync def resilient_task():\n    pass\n\n@workflow\nasync def resilient_workflow(ctx: WorkflowExecutionContext):\n    try:\n        # Task has its own error handling\n        result = await resilient_task()\n        # Workflow provides additional error boundary\n        return result\n    except Exception as e:\n        # Handle unexpected errors\n        return None\n</code></pre>"},{"location":"core-concepts/error-handling/#2-use-appropriate-timeouts","title":"2. Use Appropriate Timeouts","text":"<pre><code>@task.with_options(timeout=30)  # HTTP requests\nasync def api_task(): pass\n\n@task.with_options(timeout=300)  # File processing\nasync def processing_task(): pass\n\n@task.with_options(timeout=3600)  # Long computations\nasync def computation_task(): pass\n</code></pre>"},{"location":"core-concepts/error-handling/#3-implement-proper-cleanup","title":"3. Implement Proper Cleanup","text":"<pre><code>@task.with_options(\n    rollback=cleanup_handler,\n    fallback=fallback_handler\n)\nasync def safe_task():\n    # Cleanup handler ensures resources are released\n    # Fallback provides alternative result\n    pass\n</code></pre>"},{"location":"core-concepts/error-handling/#4-handle-partial-failures","title":"4. Handle Partial Failures","text":"<pre><code>@workflow\nasync def partial_failure_workflow(ctx: WorkflowExecutionContext):\n    results = []\n    for item in ctx.input:\n        try:\n            result = await process_item(item)\n            results.append(result)\n        except Exception as e:\n            # Log error but continue with other items\n            results.append(None)\n    return results\n</code></pre>"},{"location":"core-concepts/execution-model/","title":"Execution Model","text":""},{"location":"core-concepts/execution-model/#local-execution","title":"Local Execution","text":"<p>Local execution runs workflows directly in your Python application.</p>"},{"location":"core-concepts/execution-model/#direct-python-execution","title":"Direct Python Execution","text":"<pre><code>from flux import workflow, WorkflowExecutionContext\n\n@workflow\nasync def my_workflow(ctx: WorkflowExecutionContext[str]):\n    result = await some_task(ctx.input)\n    return result\n\n# Execute the workflow\nctx = my_workflow.run(\"input_data\")\n\n# Access results\nprint(ctx.output)\nprint(ctx.succeeded)\n</code></pre>"},{"location":"core-concepts/execution-model/#command-line-execution","title":"Command Line Execution","text":"<p>The <code>flux</code> CLI provides workflow execution through the command line:</p> <pre><code># Basic execution\nflux exec workflow_file.py workflow_name \"input_data\"\n\n# Example with hello_world workflow\nflux exec hello_world.py hello_world \"World\"\n</code></pre>"},{"location":"core-concepts/execution-model/#api-based-execution","title":"API-based Execution","text":"<p>Flux provides a built-in HTTP API server for remote workflow execution.</p>"},{"location":"core-concepts/execution-model/#starting-the-api-server","title":"Starting the API Server","text":"<pre><code># Start the server\nflux start examples\n\n# Server runs on localhost:8000 by default\n</code></pre>"},{"location":"core-concepts/execution-model/#making-api-requests","title":"Making API Requests","text":"<pre><code># Execute a workflow\ncurl --location 'localhost:8000/hello_world' \\\n     --header 'Content-Type: application/json' \\\n     --data '\"World\"'\n\n# Get execution details\ncurl --location 'localhost:8000/inspect/[execution_id]'\n</code></pre> <p>Available endpoints: - <code>POST /{workflow_name}</code> - Execute a workflow - <code>POST /{workflow_name}/{execution_id}</code> - Resume a workflow - <code>GET /inspect/{execution_id}</code> - Get execution details</p>"},{"location":"core-concepts/execution-model/#http-api-response-format","title":"HTTP API Response Format","text":"<pre><code>{\n    \"execution_id\": \"unique_execution_id\",\n    \"name\": \"workflow_name\",\n    \"input\": \"input_data\",\n    \"output\": \"result_data\"\n}\n</code></pre> <p>Add <code>?inspect=true</code> to get detailed execution information including events: <pre><code>curl --location 'localhost:8000/hello_world?inspect=true' \\\n     --header 'Content-Type: application/json' \\\n     --data '\"World\"'\n</code></pre></p>"},{"location":"core-concepts/execution-model/#execution-context","title":"Execution Context","text":"<p>The execution context maintains the state and progression of workflow execution:</p> <pre><code># Create execution context\nctx = my_workflow.run(\"input_data\")\n\n# Execution identification\nexecution_id = ctx.execution_id  # Unique identifier\nworkflow_name = ctx.name        # Workflow name\n\n# Execution state\nis_finished = ctx.finished     # Execution completed\nhas_succeeded = ctx.succeeded  # Execution succeeded\nhas_failed = ctx.failed       # Execution failed\nis_paused = ctx.paused       # Execution paused\n\n# Data access\ninput_data = ctx.input        # Input data\noutput_data = ctx.output      # Output/result data\nevent_list = ctx.events       # Execution events\n</code></pre>"},{"location":"core-concepts/execution-model/#paused-workflows","title":"Paused Workflows","text":"<p>Flux supports pausing and resuming workflows:</p> <pre><code>from flux import workflow, ExecutionContext\nfrom flux.tasks import pause\n\n@workflow\nasync def pausable_workflow(ctx: ExecutionContext):\n    # Run until the pause point\n    result = await initial_task()\n\n    # Pause execution\n    await pause(\"approval_required\")\n\n    # This code runs only after resuming\n    return await final_task(result)\n\n# Start execution (runs until pause point)\nctx = pausable_workflow.run()\nprint(f\"Paused: {ctx.paused}\")  # True\n\n# Resume execution with the same execution_id\nctx = pausable_workflow.run(execution_id=ctx.execution_id)\nprint(f\"Completed: {ctx.finished}\")  # True\n</code></pre>"},{"location":"core-concepts/execution-model/#resuming-execution","title":"Resuming Execution","text":"<pre><code># Start workflow\nctx = pausable_workflow.run()\n\n# Resume using execution ID\nctx = pausable_workflow.run(execution_id=ctx.execution_id)\n</code></pre>"},{"location":"core-concepts/execution-model/#state-management","title":"State Management","text":"<p>Flux automatically manages workflow state using SQLite for persistence. The state includes:</p> <ul> <li>Execution context</li> <li>Task results</li> <li>Events</li> <li>Execution status</li> </ul> <p>State is automatically: - Persisted after each step - Loaded when resuming execution - Used for workflow replay - Managed for error recovery</p>"},{"location":"core-concepts/execution-model/#event-system","title":"Event System","text":"<p>Events track the progression of workflow execution:</p>"},{"location":"core-concepts/execution-model/#workflow-events","title":"Workflow Events","text":"<pre><code>from flux.events import ExecutionEventType\n\n# Main workflow lifecycle\nExecutionEventType.WORKFLOW_STARTED    # Workflow begins\nExecutionEventType.WORKFLOW_COMPLETED  # Workflow succeeds\nExecutionEventType.WORKFLOW_FAILED     # Workflow fails\n</code></pre>"},{"location":"core-concepts/execution-model/#task-events","title":"Task Events","text":"<pre><code># Task lifecycle\nExecutionEventType.TASK_STARTED        # Task begins\nExecutionEventType.TASK_COMPLETED      # Task succeeds\nExecutionEventType.TASK_FAILED         # Task fails\n\n# Task retry events\nExecutionEventType.TASK_RETRY_STARTED\nExecutionEventType.TASK_RETRY_COMPLETED\nExecutionEventType.TASK_RETRY_FAILED\n\n# Task fallback events\nExecutionEventType.TASK_FALLBACK_STARTED\nExecutionEventType.TASK_FALLBACK_COMPLETED\nExecutionEventType.TASK_FALLBACK_FAILED\n\n# Task rollback events\nExecutionEventType.TASK_ROLLBACK_STARTED\nExecutionEventType.TASK_ROLLBACK_COMPLETED\nExecutionEventType.TASK_ROLLBACK_FAILED\n</code></pre>"},{"location":"core-concepts/execution-model/#accessing-events","title":"Accessing Events","text":"<pre><code># Get all events\nfor event in ctx.events:\n    print(f\"Event: {event.type}\")\n    print(f\"Time: {event.time}\")\n    print(f\"Value: {event.value}\")\n\n# Get last event\nlast_event = ctx.events[-1]\n</code></pre>"},{"location":"core-concepts/tasks/","title":"Task System","text":""},{"location":"core-concepts/tasks/#task-creation","title":"Task Creation","text":"<p>Tasks in Flux are Python functions decorated with <code>@task</code>. They represent individual units of work that can be composed into workflows.</p>"},{"location":"core-concepts/tasks/#basic-task-creation","title":"Basic Task Creation","text":"<pre><code>from flux import task\n\n@task\nasync def simple_task(data: str):\n    return data.upper()\n</code></pre>"},{"location":"core-concepts/tasks/#configured-task-creation","title":"Configured Task Creation","text":"<pre><code>@task.with_options(\n    name=\"process_data\",                 # Custom task name\n    retry_max_attempts=3,                # Maximum retry attempts\n    retry_delay=1,                       # Initial delay between retries\n    retry_backoff=2,                     # Backoff multiplier\n    timeout=30,                          # Task timeout in seconds\n    fallback=fallback_function,          # Fallback function\n    rollback=rollback_function,          # Rollback function\n    secret_requests=[\"API_KEY\"],         # Required secrets\n    output_storage=custom_storage        # Custom output storage\n)\nasync def complex_task(data: str, secrets: dict = {}):\n    # Task implementation using secrets\n    return process_data(data, secrets[\"API_KEY\"])\n</code></pre>"},{"location":"core-concepts/tasks/#task-options","title":"Task Options","text":""},{"location":"core-concepts/tasks/#retry-configuration","title":"Retry Configuration","text":"<pre><code>@task.with_options(\n    retry_max_attempts=3,    # Try up to 3 times\n    retry_delay=1,          # Wait 1 second initially\n    retry_backoff=2         # Double delay after each retry\n)\ndef retrying_task():\n    # Task will retry with delays: 1s, 2s, 4s\n    pass\n</code></pre>"},{"location":"core-concepts/tasks/#timeout-configuration","title":"Timeout Configuration","text":"<pre><code>@task.with_options(timeout=30)\ndef timed_task():\n    # Task will fail if it exceeds 30 seconds\n    pass\n</code></pre>"},{"location":"core-concepts/tasks/#fallback-configuration","title":"Fallback Configuration","text":"<pre><code>def fallback_handler(input_data):\n    # Handle task failure\n    return \"fallback result\"\n\n@task.with_options(fallback=fallback_handler)\ndef task_with_fallback(input_data):\n    # If this fails, fallback_handler is called\n    pass\n</code></pre>"},{"location":"core-concepts/tasks/#rollback-configuration","title":"Rollback Configuration","text":"<pre><code>def rollback_handler(input_data):\n    # Clean up after task failure\n    pass\n\n@task.with_options(rollback=rollback_handler)\ndef task_with_rollback(input_data):\n    # If this fails, rollback_handler is called\n    pass\n</code></pre>"},{"location":"core-concepts/tasks/#task-composition","title":"Task Composition","text":"<p>Flux provides several ways to compose tasks:</p>"},{"location":"core-concepts/tasks/#parallel-execution","title":"Parallel Execution","text":"<pre><code>from flux.tasks import parallel\n\n@workflow\nasync def parallel_workflow(ctx: WorkflowExecutionContext):\n    results = await parallel(\n        task1(),\n        task2(),\n        task3()\n    )\n    return results\n</code></pre>"},{"location":"core-concepts/tasks/#pipeline-processing","title":"Pipeline Processing","text":"<pre><code>from flux.tasks import pipeline\n\n@workflow\nasync def pipeline_workflow(ctx: WorkflowExecutionContext):\n    result = await pipeline(\n        task1,\n        task2,\n        task3,\n        input=ctx.input\n    )\n    return result\n</code></pre>"},{"location":"core-concepts/tasks/#task-mapping","title":"Task Mapping","text":"<pre><code>@task\nasync def process_item(item: str):\n    return item.upper()\n\n@workflow\nasync def mapping_workflow(ctx: WorkflowExecutionContext):\n    items = [\"a\", \"b\", \"c\"]\n    results = await process_item.map(items)\n    return results\n</code></pre>"},{"location":"core-concepts/tasks/#error-handling","title":"Error Handling","text":"<p>Tasks support multiple error handling strategies that can be combined:</p>"},{"location":"core-concepts/tasks/#retry-mechanism","title":"Retry Mechanism","text":"<pre><code>@task.with_options(\n    retry_max_attempts=3,\n    retry_delay=1,\n    retry_backoff=2\n)\ndef retrying_task():\n    if random.random() &lt; 0.5:\n        raise ValueError(\"Task failed\")\n    return \"success\"\n</code></pre>"},{"location":"core-concepts/tasks/#fallback-strategy","title":"Fallback Strategy","text":"<pre><code>def fallback_handler():\n    return \"fallback result\"\n\n@task.with_options(\n    fallback=fallback_handler,\n    retry_max_attempts=3\n)\ndef task_with_fallback():\n    # Retries first, then fallback if all retries fail\n    pass\n</code></pre>"},{"location":"core-concepts/tasks/#rollback-operations","title":"Rollback Operations","text":"<pre><code>def rollback_handler():\n    # Clean up resources\n    pass\n\n@task.with_options(rollback=rollback_handler)\ndef task_with_rollback():\n    # Rollback is called if task fails\n    pass\n</code></pre>"},{"location":"core-concepts/tasks/#timeout-handling","title":"Timeout Handling","text":"<pre><code>@task.with_options(\n    timeout=30,\n    fallback=fallback_handler\n)\ndef timed_task():\n    # Fails if exceeds 30 seconds, then calls fallback\n    pass\n</code></pre>"},{"location":"core-concepts/tasks/#built-in-tasks","title":"Built-in Tasks","text":"<p>Flux provides several built-in tasks for common operations:</p>"},{"location":"core-concepts/tasks/#time-operations","title":"Time Operations","text":"<pre><code>from flux.tasks import now, sleep\n\n@workflow\nasync def timing_workflow(ctx: WorkflowExecutionContext):\n    start_time = await now()           # Get current time\n    await sleep(timedelta(seconds=5))   # Sleep for duration\n    end_time = await now()\n    return end_time - start_time\n</code></pre>"},{"location":"core-concepts/tasks/#random-operations","title":"Random Operations","text":"<pre><code>from flux.tasks import choice, randint, randrange\n\n@workflow\nasync def random_workflow(ctx: WorkflowExecutionContext):\n    chosen = await choice([\"a\", \"b\", \"c\"])    # Random choice\n    number = await randint(1, 10)             # Random integer\n    range_num = await randrange(0, 10, 2)     # Random range\n</code></pre>"},{"location":"core-concepts/tasks/#uuid-generation","title":"UUID Generation","text":"<pre><code>from flux.tasks import uuid4\n\n@workflow\nasync def id_workflow(ctx: WorkflowExecutionContext):\n    new_id = await uuid4()  # Generate UUID\n</code></pre>"},{"location":"core-concepts/tasks/#workflow-pauses","title":"Workflow Pauses","text":"<pre><code>from flux.tasks import pause\n\n@workflow\nasync def approval_workflow(ctx: WorkflowExecutionContext):\n    # Process something\n    result = await process_data()\n\n    # Pause the workflow with a named pause point\n    await pause(\"wait_for_approval\")\n\n    # This code will only execute after the workflow is resumed\n    return f\"Approved: {result}\"\n</code></pre>"},{"location":"core-concepts/tasks/#graph-based-task-composition","title":"Graph-based Task Composition","text":"<p>The Graph task allows you to create complex task dependencies using a directed acyclic graph (DAG):</p> <pre><code>from flux.tasks import Graph\n\n@task\ndef get_name(input: str) -&gt; str:\n    return input\n\n@task\ndef say_hello(name: str) -&gt; str:\n    return f\"Hello, {name}\"\n\n@workflow\nasync def graph_workflow(ctx: WorkflowExecutionContext[str]):\n    # Create a graph named \"hello_world\"\n    hello = (\n        Graph(\"hello_world\")\n        # Add nodes (tasks)\n        .add_node(\"get_name\", get_name)\n        .add_node(\"say_hello\", say_hello)\n        # Define edges (dependencies)\n        .add_edge(\"get_name\", \"say_hello\")\n        # Define entry and exit points\n        .start_with(\"get_name\")\n        .end_with(\"say_hello\")\n    )\n\n    # Execute the graph\n    return await hello(ctx.input)\n</code></pre> <p>Graph features: - Create complex task dependencies - Define conditional execution paths - Validate graph structure - Automatic task ordering</p>"},{"location":"core-concepts/tasks/#graph-construction","title":"Graph Construction","text":"<pre><code># Create a more complex graph with conditions\ngraph = (\n    Graph(\"complex_workflow\")\n    # Add nodes\n    .add_node(\"task1\", task1_func)\n    .add_node(\"task2\", task2_func)\n    .add_node(\"task3\", task3_func)\n\n    # Add edges with conditions\n    .add_edge(\"task1\", \"task2\", condition=lambda result: result &gt; 0)\n    .add_edge(\"task1\", \"task3\", condition=lambda result: result &lt;= 0)\n\n    # Define workflow boundaries\n    .start_with(\"task1\")\n    .end_with(\"task2\")\n    .end_with(\"task3\")\n\n    # Validate graph structure\n    .validate()\n)\n</code></pre>"},{"location":"core-concepts/tasks/#graph-properties","title":"Graph Properties","text":"<ul> <li>Nodes can have custom actions</li> <li>Edges can have conditions</li> <li>Automatic cycle detection</li> <li>Guaranteed execution order</li> <li>Built-in validation</li> </ul>"},{"location":"core-concepts/tasks/#error-handling-in-graphs","title":"Error Handling in Graphs","text":"<pre><code>def check_condition(result):\n    return isinstance(result, str)\n\ngraph = (\n    Graph(\"error_handling\")\n    .add_node(\"task1\", safe_task)\n    .add_node(\"fallback\", fallback_task)\n    .add_edge(\"task1\", \"fallback\", condition=lambda r: not check_condition(r))\n    .start_with(\"task1\")\n    .end_with(\"fallback\")\n)\n</code></pre>"},{"location":"core-concepts/workflow-management/","title":"Workflow Management","text":""},{"location":"core-concepts/workflow-management/#creating-workflows","title":"Creating Workflows","text":"<p>A workflow in Flux is created by combining the <code>@workflow</code> decorator with a Python async function that uses await for tasks. Workflows are the primary way to organize and orchestrate task execution.</p> <pre><code>from flux import workflow, ExecutionContext, task\n\n@task\nasync def process_data(data: str):\n    return data.upper()\n\n@workflow\nasync def my_workflow(ctx: ExecutionContext[str]):\n    # Workflows must take an ExecutionContext as first argument\n    # The type parameter [str] indicates the expected input type\n    result = await process_data(ctx.input)\n    return result\n</code></pre>"},{"location":"core-concepts/workflow-management/#workflow-configuration","title":"Workflow Configuration","text":"<p>Workflows can be configured using <code>with_options</code>:</p> <pre><code>@workflow.with_options(\n    name=\"custom_workflow\",              # Custom name for the workflow\n    secret_requests=[\"API_KEY\"],         # Secrets required by the workflow\n    output_storage=custom_storage        # Custom storage for workflow outputs\n)\nasync def configured_workflow(ctx: ExecutionContext):\n    pass\n</code></pre>"},{"location":"core-concepts/workflow-management/#workflow-lifecycle","title":"Workflow Lifecycle","text":"<p>A workflow goes through several stages during its execution:</p> <ol> <li> <p>Initialization <pre><code># Workflow is registered with a unique execution ID\nctx = my_workflow.run(\"input_data\")\n</code></pre></p> </li> <li> <p>Execution <pre><code>@workflow\nasync def lifecycle_example(ctx: ExecutionContext):\n    # Start event is generated\n    first_result = await task_one()    # Task execution\n    second_result = await task_two()    # Next task\n    return second_result                # Completion\n</code></pre></p> </li> <li> <p>Completion or Failure <pre><code># Check workflow status\nif ctx.has_finished:\n    if ctx.has_succeeded:\n        print(f\"Success: {ctx.output}\")\n    elif ctx.has_failed:\n        print(f\"Failed: {ctx.output}\")  # Contains error information\n</code></pre></p> </li> <li> <p>Replay or Resume (if needed)    <pre><code># Resume a previous execution\nctx = my_workflow.run(execution_id=previous_execution_id)\n</code></pre></p> </li> </ol>"},{"location":"core-concepts/workflow-management/#workflow-states","title":"Workflow States","text":"<p>Workflows can be in several states:</p> <ol> <li> <p>Running <pre><code>ctx = my_workflow.run(\"input\")\nprint(ctx.has_finished)  # False while running\n</code></pre></p> </li> <li> <p>Paused <pre><code># Workflow with pause point\nfrom flux.tasks import pause\n\n@workflow\nasync def pausable_workflow(ctx: ExecutionContext):\n    await some_task()\n    await pause(\"manual_approval\")\n    return \"Complete\"\n\nctx = pausable_workflow.run()\nprint(ctx.is_paused)  # True when paused\nprint(ctx.has_finished)  # False when paused\n\n# Resume paused workflow\nctx = pausable_workflow.run(execution_id=ctx.execution_id)\n</code></pre></p> </li> <li> <p>Completed <pre><code># Successfully completed\nprint(ctx.has_finished and ctx.has_succeeded)  # True\nprint(ctx.output)  # Contains workflow result\n</code></pre></p> </li> <li> <p>Failed <pre><code># Failed execution\nprint(ctx.has_finished and ctx.has_failed)  # True\nprint(ctx.output)  # Contains error information\n</code></pre></p> </li> </ol>"},{"location":"getting-started/basic_concepts/","title":"Basic Concepts","text":""},{"location":"getting-started/basic_concepts/#workflows","title":"Workflows","text":"<p>A workflow in Flux is a Python function that orchestrates a series of tasks to achieve a specific goal. Workflows are defined using the <code>@workflow</code> decorator and provide a high-level way to organize and manage task execution.</p> <pre><code>from flux import workflow, ExecutionContext\nfrom flux.tasks import pause\n\n# Basic workflow\n@workflow\nasync def my_workflow(ctx: ExecutionContext[str]):\n    result = await some_task(ctx.input)\n    return result\n\n# Configured workflow\n@workflow.with_options(\n    name=\"custom_workflow\",              # Custom workflow name\n    secret_requests=[\"API_KEY\"],         # Required secrets\n    output_storage=custom_storage        # Custom output storage\n)\nasync def configured_workflow(ctx: ExecutionContext):\n    result = await some_task()\n    return result\n\n# Workflow with pause point\n@workflow\nasync def approval_workflow(ctx: ExecutionContext):\n    data = await process_data(ctx.input)\n    # Pause for manual approval\n    await pause(\"manual_approval\")\n    # Continues after workflow is resumed\n    return f\"Approved: {data}\"\n</code></pre> <p>Key characteristics of workflows: - Must be decorated with <code>@workflow</code> or <code>@workflow.with_options()</code> - Take an <code>ExecutionContext</code> as their first argument - Use <code>async/await</code> to execute tasks asynchronously - Can be run directly, via CLI, or through HTTP API - Maintain execution state between runs - Support pause and resume operations for manual interventions</p>"},{"location":"getting-started/basic_concepts/#tasks","title":"Tasks","text":"<p>Tasks are the basic units of work in Flux. They are Python functions decorated with <code>@task</code> that perform specific operations within a workflow.</p> <pre><code>from flux import task\n\n# Basic task\n@task\nasync def simple_task(data: str):\n    return data.upper()\n\n# Configured task\n@task.with_options(\n    name=\"custom_task\",                  # Custom task name\n    retry_max_attempts=3,                 # Maximum retry attempts\n    retry_delay=1,                       # Initial delay between retries\n    retry_backoff=2,                     # Backoff multiplier for retries\n    timeout=30,                          # Task timeout in seconds\n    fallback=fallback_function,          # Fallback function for failures\n    rollback=rollback_function,          # Rollback function for failures\n    secret_requests=[\"API_KEY\"],         # Required secrets\n    output_storage=custom_storage        # Custom output storage\n)\nasync def complex_task(data: str):\n    return process_data(data)\n</code></pre> <p>Task features: - Basic tasks with <code>@task</code> decorator - Configurable options:   - <code>retry_max_attempts</code>: Maximum retry attempts   - <code>retry_delay</code>: Initial delay between retries   - <code>retry_backoff</code>: Backoff multiplier for subsequent retries   - <code>timeout</code>: Task execution timeout   - <code>fallback</code>: Fallback function for handling failures - Can be composed and nested - Support for parallel execution and mapping operations</p>"},{"location":"getting-started/basic_concepts/#execution-context","title":"Execution Context","text":"<p>The <code>ExecutionContext</code> is a container that maintains the state and information about a workflow execution.</p> <pre><code>from flux import ExecutionContext\n\n@workflow\nasync def example_workflow(ctx: ExecutionContext[str]):\n    # Access context properties\n    execution_id = ctx.execution_id     # Unique execution identifier\n    input_data = ctx.input             # Workflow input\n    is_finished = ctx.has_finished     # Execution status\n    has_succeeded = ctx.has_succeeded  # Success status\n    output_data = ctx.output          # Workflow output\n</code></pre> <p>Context properties: - <code>execution_id</code>: Unique identifier for the workflow execution - <code>name</code>: Name of the workflow - <code>input</code>: Input data provided to the workflow - <code>events</code>: List of execution events - <code>has_finished</code>: Whether the workflow has completed - <code>has_succeeded</code>: Whether the workflow completed successfully - <code>has_failed</code>: Whether the workflow failed - <code>is_paused</code>: Whether the workflow is currently paused - <code>output</code>: Final output of the workflow</p>"},{"location":"getting-started/basic_concepts/#events","title":"Events","text":"<p>Events track the progress and state changes during workflow execution. Flux automatically generates events for various workflow and task operations.</p> <pre><code>from flux.domain.events import ExecutionEventType\n\n# Example of event types\nExecutionEventType.WORKFLOW_STARTED    # Workflow begins execution\nExecutionEventType.WORKFLOW_COMPLETED  # Workflow completes successfully\nExecutionEventType.WORKFLOW_PAUSED    # Workflow is paused\nExecutionEventType.TASK_STARTED       # Task begins execution\nExecutionEventType.TASK_COMPLETED     # Task completes successfully\nExecutionEventType.TASK_PAUSED       # Task is paused\n</code></pre> <p>Event categories: 1. Workflow Events:    - <code>WORKFLOW_STARTED</code>    - <code>WORKFLOW_COMPLETED</code>    - <code>WORKFLOW_FAILED</code>    - <code>WORKFLOW_PAUSED</code></p> <ol> <li>Task Events:</li> <li><code>TASK_STARTED</code></li> <li><code>TASK_COMPLETED</code></li> <li><code>TASK_FAILED</code></li> <li><code>TASK_PAUSED</code></li> <li><code>TASK_RETRY_STARTED</code></li> <li><code>TASK_RETRY_COMPLETED</code></li> <li><code>TASK_RETRY_FAILED</code></li> <li><code>TASK_FALLBACK_STARTED</code></li> <li><code>TASK_FALLBACK_COMPLETED</code></li> <li><code>TASK_FALLBACK_FAILED</code></li> <li><code>TASK_ROLLBACK_STARTED</code></li> <li><code>TASK_ROLLBACK_COMPLETED</code></li> <li><code>TASK_ROLLBACK_FAILED</code></li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>Before installing Flux, ensure you have:</p> <ul> <li>Python 3.12 or later</li> <li>pip (Python package installer)</li> <li>Optional: Poetry for dependency management</li> </ul>"},{"location":"getting-started/installation/#core-dependencies","title":"Core Dependencies","text":"<p>Flux relies on the following packages: <pre><code>pydantic = \"^2.9.2\"\nsqlalchemy = \"^2.0.35\"\nfastapi = \"^0.115.2\"\nuvicorn = \"^0.31.1\"\npycryptodome = \"^3.21.0\"\n</code></pre></p>"},{"location":"getting-started/installation/#storage-requirements","title":"Storage Requirements","text":"<ul> <li>Write access to create a <code>.data</code> directory for SQLite database storage</li> <li>Sufficient disk space for state persistence</li> </ul>"},{"location":"getting-started/installation/#installation-guide","title":"Installation Guide","text":""},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install flux-core\n</code></pre>"},{"location":"getting-started/installation/#using-poetry","title":"Using Poetry","text":"<pre><code># Initialize a new project\npoetry init\n\n# Add Flux as a dependency\npoetry add flux-core\n\n# Enter the virtual environment\npoetry shell\n</code></pre>"},{"location":"getting-started/installation/#installing-for-development","title":"Installing for Development","text":"<p>If you're planning to contribute or need development tools: <pre><code>git clone https://github.com/edurdias/flux\ncd flux\npoetry install\n</code></pre></p> <p>This will install additional development dependencies for testing and code quality: - pytest and related plugins for testing - pylint, pyright, and other linting tools - pre-commit hooks for code quality</p>"},{"location":"getting-started/installation/#quick-setup","title":"Quick Setup","text":"<ol> <li>Verify Installation Check that Flux is properly installed by creating a simple test workflow:</li> </ol> <pre><code>from flux import task, workflow, WorkflowExecutionContext\n\n@task\nasync def say_hello(name: str):\n    return f\"Hello, {name}\"\n\n@workflow\nasync def hello_world(ctx: WorkflowExecutionContext[str]):\n    return await say_hello(ctx.input)\n</code></pre> <ol> <li>Run the Workflow You can execute workflows in three ways:</li> </ol> <pre><code># Python\nctx = hello_world.run(\"World\")\nprint(ctx.output)\n\n# Command Line\nflux exec hello_world.py hello_world \"World\"\n\n# HTTP API\nflux start examples\ncurl --location 'localhost:8000/hello_world' \\\n     --header 'Content-Type: application/json' \\\n     --data '\"World\"'\n</code></pre> <ol> <li> <p>Directory Structure Flux will automatically create its required directories: <pre><code>your-project/\n\u251c\u2500\u2500 .data/          # SQLite database and state storage\n\u2514\u2500\u2500 your_workflows/\n</code></pre></p> </li> <li> <p>Next Steps</p> </li> <li>Create your first workflow using the examples</li> <li>Learn about core concepts</li> <li>Explore advanced features</li> </ol>"},{"location":"getting-started/quick-start-guide/","title":"Quick Start Guide","text":""},{"location":"getting-started/quick-start-guide/#first-workflow","title":"First Workflow","text":"<p>Let's start with a simple \"Hello World\" workflow:</p> <pre><code>from flux import task, workflow, ExecutionContext\n\n@task\nasync def say_hello(name: str):\n    return f\"Hello, {name}\"\n\n@workflow\nasync def hello_world(ctx: ExecutionContext[str]):\n    if not ctx.input:\n        raise TypeError(\"Input not provided\")\n    return await say_hello(ctx.input)\n</code></pre> <p>This simple example shows: - A basic task with the <code>@task</code> decorator - A workflow with the <code>@workflow</code> decorator - Basic error handling for missing input - Usage of <code>async/await</code> for asynchronous execution</p>"},{"location":"getting-started/quick-start-guide/#running-workflows","title":"Running Workflows","text":"<p>You can run workflows in three different ways:</p>"},{"location":"getting-started/quick-start-guide/#1-direct-python-execution","title":"1. Direct Python Execution","text":"<pre><code># Run the workflow directly\nctx = hello_world.run(\"World\")\nprint(ctx.output)  # Prints: Hello, World\n\n# Check execution status\nprint(ctx.has_finished)   # True if workflow completed\nprint(ctx.has_succeeded)  # True if workflow succeeded\n</code></pre>"},{"location":"getting-started/quick-start-guide/#2-command-line-interface","title":"2. Command Line Interface","text":"<pre><code># Execute workflow using the CLI\nflux exec hello_world.py hello_world \"World\"\n</code></pre>"},{"location":"getting-started/quick-start-guide/#3-http-api","title":"3. HTTP API","text":"<pre><code># Start the API server\nflux start examples\n\n# Execute workflow via HTTP\ncurl --location 'localhost:8000/hello_world' \\\n     --header 'Content-Type: application/json' \\\n     --data '\"World\"'\n</code></pre>"},{"location":"introduction/features/","title":"Key Features","text":""},{"location":"introduction/features/#high-performance-task-execution","title":"High-Performance Task Execution","text":"<ul> <li>Parallel Execution: Execute multiple tasks concurrently using built-in parallel processing</li> <li>Task Mapping: Apply operations across collections of data efficiently</li> <li>Pipeline Processing: Chain tasks together in efficient processing pipelines</li> <li>Graph-based Workflows: Create complex task dependencies using directed acyclic graphs</li> </ul>"},{"location":"introduction/features/#fault-tolerance","title":"Fault-Tolerance","text":"<ul> <li>Automatic Retries: Configure retry attempts with customizable backoff strategies</li> <li>Fallback Mechanisms: Define fallback behavior for failed tasks</li> <li>Error Recovery: Roll back failed operations with custom recovery logic</li> <li>Task Timeouts: Set execution time limits to prevent hanging tasks</li> </ul>"},{"location":"introduction/features/#durable-execution","title":"Durable Execution","text":"<ul> <li>State Persistence: Maintain workflow state across executions</li> <li>Checkpoint Support: Create save points in long-running workflows</li> <li>Resume Capability: Continue workflows from their last successful state</li> <li>Event Tracking: Monitor and log all workflow and task events</li> </ul>"},{"location":"introduction/features/#workflow-controls","title":"Workflow Controls","text":"<ul> <li>Pause/Resume: Pause workflows at defined points and resume when ready</li> <li>State Inspection: Examine workflow state at any point during execution</li> <li>Workflow Replay: Replay workflows for debugging or recovery</li> <li>Subworkflow Support: Compose complex workflows from simpler ones</li> </ul>"},{"location":"introduction/features/#security","title":"Security","text":"<ul> <li>Secret Management: Securely handle sensitive data during workflow execution</li> <li>Encrypted Storage: Protect sensitive data at rest</li> <li>Access Control: Manage who can execute and modify workflows</li> </ul>"},{"location":"introduction/features/#api-integration","title":"API Integration","text":"<ul> <li>HTTP API: Built-in FastAPI server for HTTP access</li> <li>RESTful Endpoints: Easy-to-use REST API for workflow management</li> <li>Programmatic Access: Python API for direct integration</li> </ul>"},{"location":"introduction/features/#development-features","title":"Development Features","text":"<ul> <li>Type Safety: Full type hinting support for better development experience</li> <li>Testing Support: Comprehensive testing utilities for workflows</li> <li>Debugging Tools: Rich debugging information and state inspection</li> <li>Local Development: Easy local development and testing workflow</li> </ul>"},{"location":"introduction/overview/","title":"Overview","text":"<p>Flux is a distributed workflow orchestration engine designed to build stateful and fault-tolerant workflows in Python. It provides a robust framework for creating, managing, and executing complex workflows with built-in support for error handling, retries, and state management.</p>"},{"location":"introduction/overview/#what-is-flux","title":"What is Flux?","text":"<p>Flux is a Python-based workflow orchestration system that allows you to: - Define workflows as Python code using decorators - Break down complex processes into manageable tasks - Handle failures and retries automatically - Maintain state across workflow executions - Scale from local development to distributed systems</p>"},{"location":"introduction/use-cases/","title":"Use Cases","text":"<p>Flux is particularly well-suited for:</p>"},{"location":"introduction/use-cases/#data-processing-pipelines","title":"Data Processing Pipelines","text":"<ul> <li>ETL workflows</li> <li>Data transformation</li> <li>Batch processing</li> <li>Stream processing</li> </ul>"},{"location":"introduction/use-cases/#integration-workflows","title":"Integration Workflows","text":"<ul> <li>API orchestration</li> <li>Service integration</li> <li>Event-driven processes</li> <li>Microservice coordination</li> </ul>"},{"location":"introduction/use-cases/#business-process-automation","title":"Business Process Automation","text":"<ul> <li>Order processing</li> <li>Document handling</li> <li>Approval workflows</li> <li>Multi-step operations</li> </ul>"},{"location":"introduction/use-cases/#machine-learning-operations","title":"Machine Learning Operations","text":"<ul> <li>Model training pipelines</li> <li>Data preparation workflows</li> <li>Feature engineering</li> <li>Model deployment processes</li> </ul>"}]}